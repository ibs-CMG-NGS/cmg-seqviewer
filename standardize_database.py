"""
Database Standardization Script

ê¸°ì¡´ database íŒŒì¼ë“¤ì„ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¬ì €ì¥í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
sys.path.insert(0, 'src')

import pandas as pd
from utils.database_manager import DatabaseManager
from utils.data_loader import DataLoader
from models.data_models import DatasetType
from models.standard_columns import StandardColumns

def standardize_database():
    """ëª¨ë“  database íŒŒì¼ì„ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€í™˜"""
    
    db_manager = DatabaseManager()
    loader = DataLoader()
    
    # ëª¨ë“  dataset ë©”íƒ€ë°ì´í„° ë¡œë“œ
    all_metadata = db_manager.get_all_metadata()
    
    print(f"Found {len(all_metadata)} datasets to standardize")
    print("=" * 70)
    
    converted = 0
    skipped = 0
    errors = 0
    
    for metadata in all_metadata:
        try:
            print(f"\nProcessing: {metadata.alias}")
            
            # Parquet íŒŒì¼ ë¡œë“œ
            file_path = db_manager.datasets_dir / Path(metadata.file_path).name
            
            if not file_path.exists():
                print(f"  âŒ File not found: {file_path}")
                errors += 1
                continue
            
            df = pd.read_parquet(file_path)
            print(f"  ğŸ“Š Original columns: {list(df.columns)[:10]}")
            
            # ì´ë¯¸ í‘œì¤€í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (gene_id, symbol, lfcse, stat ëª¨ë‘ ì²´í¬)
            required_standard = [StandardColumns.GENE_ID, StandardColumns.SYMBOL, StandardColumns.LOG2FC, 
                                 StandardColumns.ADJ_PVALUE, StandardColumns.LFCSE, StandardColumns.STAT]
            if all(col in df.columns for col in required_standard):
                print(f"  âœ… Already standardized, skipping...")
                skipped += 1
                continue
            
            # í‘œì¤€í™” ìˆ˜í–‰
            auto_mapping = loader._map_columns(df, metadata.dataset_type)
            df_std, original_columns = loader._standardize_columns(df, auto_mapping, metadata.dataset_type)
            
            print(f"  ğŸ”„ Standardized columns: {list(df_std.columns)[:10]}")
            print(f"  ğŸ“ Mapping: {original_columns}")
            
            # ë™ì¼í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë®ì–´ì“°ê¸°
            df_std.to_parquet(file_path, engine='pyarrow', compression='snappy')
            print(f"  âœ… Saved standardized data")
            
            converted += 1
            
        except Exception as e:
            print(f"  âŒ Error: {e}")
            errors += 1
    
    print("\n" + "=" * 70)
    print(f"\nğŸ“Š Summary:")
    print(f"  âœ… Converted: {converted}")
    print(f"  â­ï¸  Skipped (already standard): {skipped}")
    print(f"  âŒ Errors: {errors}")
    print(f"  ğŸ“ Total: {len(all_metadata)}")
    
    if converted > 0:
        print(f"\nâœ¨ Successfully standardized {converted} database files!")
        print(f"   Database files now use standard column names.")
    
    if errors > 0:
        print(f"\nâš ï¸  {errors} files had errors during conversion.")
    
    return converted, skipped, errors

if __name__ == "__main__":
    print("ğŸ”§ CMG-SeqViewer Database Standardization")
    print("=" * 70)
    print("\nThis script will convert all database files to use standard column names.")
    print("Original files will be overwritten with standardized versions.\n")
    
    response = input("Continue? (y/n): ")
    if response.lower() != 'y':
        print("âŒ Cancelled by user")
        sys.exit(0)
    
    print("\nğŸš€ Starting standardization...")
    converted, skipped, errors = standardize_database()
    
    if errors == 0:
        print("\nâœ… Database standardization complete!")
        print("   You can now run the program without conversion overhead.")
    else:
        print("\nâš ï¸  Standardization completed with some errors.")
        print("   Check the output above for details.")
