"""
Main Presenter for RNA-Seq Data Analysis Program

MVP (Model-View-Presenter) íŒ¨í„´ì˜ Presenter êµ¬í˜„
GUI ë¡œì§ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
"""

import logging
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd
from PyQt6.QtCore import QObject, pyqtSignal, Qt

from core.fsm import FSM, State, Event
from core.logger import get_audit_logger
from models.data_models import Dataset, FilterCriteria, ComparisonResult, FilterMode, DatasetType
from models.standard_columns import StandardColumns
from utils.data_loader import DataLoader
from utils.statistics import StatisticalAnalyzer
from gui.workers import DataLoadWorker, FilterWorker, AnalysisWorker


class MainPresenter(QObject):
    """
    ë©”ì¸ Presenter
    
    View(GUI)ì™€ Model(ë°ì´í„°/ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§) ì‚¬ì´ì˜ ì¤‘ì¬ì ì—­í• ì„ í•©ë‹ˆë‹¤.
    FSMì„ í†µí•´ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    
    # Signals
    dataset_loaded = pyqtSignal(str, Dataset)  # dataset_name, dataset
    filter_completed = pyqtSignal(pd.DataFrame, str)  # filtered_data, tab_name
    analysis_completed = pyqtSignal(dict, str)  # result, analysis_type
    comparison_completed = pyqtSignal(ComparisonResult)
    error_occurred = pyqtSignal(str)  # error_message
    progress_updated = pyqtSignal(int)  # progress percentage
    
    def __init__(self, view):
        """
        Args:
            view: MainWindow ì¸ìŠ¤í„´ìŠ¤
        """
        super().__init__()
        
        self.view = view
        self.logger = logging.getLogger(__name__)
        self.audit_logger = get_audit_logger()
        
        # FSM ì´ˆê¸°í™”
        self.fsm = FSM(initial_state=State.IDLE)
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.datasets: Dict[str, Dataset] = {}
        self.current_dataset: Optional[Dataset] = None
        
        # ìœ í‹¸ë¦¬í‹°
        self.data_loader = DataLoader()
        self.analyzer = StatisticalAnalyzer()
        
        # FSM ì½œë°± ë“±ë¡
        self._register_fsm_callbacks()
        
        # Worker ì°¸ì¡° (ë¹„ë™ê¸° ì‘ì—…)
        self.active_workers: List[QObject] = []
    
    def _register_fsm_callbacks(self):
        """FSM ìƒíƒœ ì§„ì…/ì´íƒˆ ì½œë°± ë“±ë¡"""
        # LOADING_DATA ì§„ì… ì‹œ
        self.fsm.register_on_enter(State.LOADING_DATA, self._on_loading_started)
        
        # FILTERING ì§„ì… ì‹œ
        self.fsm.register_on_enter(State.FILTERING, self._on_filtering_started)
        
        # ANALYZING ì§„ì… ì‹œ
        self.fsm.register_on_enter(State.ANALYZING, self._on_analyzing_started)
        
        # ERROR ì§„ì… ì‹œ
        self.fsm.register_on_enter(State.ERROR, self._on_error_state)
    
    def _on_loading_started(self, **kwargs):
        """ë°ì´í„° ë¡œë”© ì‹œì‘"""
        self.logger.info("Data loading started")
    
    def _on_filtering_started(self, **kwargs):
        """í•„í„°ë§ ì‹œì‘"""
        self.logger.info("Filtering started")
    
    def _on_analyzing_started(self, **kwargs):
        """ë¶„ì„ ì‹œì‘"""
        self.logger.info("Analysis started")
    
    def _on_error_state(self, **kwargs):
        """ì˜¤ë¥˜ ìƒíƒœ ì§„ì…"""
        error_msg = kwargs.get('error_message', 'Unknown error occurred')
        self.logger.error(f"Error state: {error_msg}")
        self.error_occurred.emit(error_msg)
    
    def load_dataset(self, file_path: Path, dataset_name: Optional[str] = None, custom_name: Optional[str] = None):
        """
        ë°ì´í„°ì…‹ ë¡œë“œ (ë¹„ë™ê¸°)
        
        Args:
            file_path: Excel íŒŒì¼ ê²½ë¡œ
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (Noneì´ë©´ íŒŒì¼ëª… ì‚¬ìš©) - deprecated, use custom_name
            custom_name: ì‚¬ìš©ì ì§€ì • ì´ë¦„ (ìš°ì„ ìˆœìœ„ ìµœìƒìœ„)
        """
        import time
        start_time = time.time()
        
        # custom_nameì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        final_name = custom_name or dataset_name
        
        # ìƒíƒœ ì „í™˜
        if not self.fsm.trigger(Event.LOAD_DATA):
            self.logger.warning("Cannot load data in current state")
            return
        
        # Audit log
        self.audit_logger.log_action(
            "Load Dataset",
            details={'file': str(file_path), 'name': final_name or file_path.stem}
        )
        
        try:
            # ë¨¼ì € íŒŒì¼ íƒ€ì…ì„ ë¹ ë¥´ê²Œ ê°ì§€ (ì²« ë²ˆì§¸ ì‹œíŠ¸ë§Œ í™•ì¸)
            try:
                test_df = pd.read_excel(file_path, nrows=10)  # ì²« 10í–‰ë§Œ ì½ì–´ì„œ íƒ€ì… ê°ì§€
                detected_type = self.data_loader._detect_dataset_type(test_df)
                self.logger.info(f"Quick type detection: {detected_type.value}")
                
                # GO/KEGG íƒ€ì…ì´ë©´ ì „ìš© ë¡œë” ì‚¬ìš©
                if detected_type == DatasetType.GO_ANALYSIS:
                    from utils.go_kegg_loader import GOKEGGLoader
                    loader = GOKEGGLoader()
                    dataset = loader.load_from_excel(file_path, final_name or file_path.stem)
                    
                    # ë°ì´í„°ì…‹ ì €ì¥
                    unique_name = self.view.dataset_manager._generate_unique_name(dataset.name)
                    dataset.name = unique_name
                    self.datasets[unique_name] = dataset
                    self.current_dataset = dataset
                    
                    # ìƒíƒœ ì „í™˜
                    self.fsm.trigger(Event.DATA_LOAD_SUCCESS)
                    
                    # Signal ë°©ì¶œ
                    self.dataset_loaded.emit(dataset.name, dataset)
                    
                    # GUI ì—…ë°ì´íŠ¸
                    self._update_view_with_dataset(dataset)
                    self.view._update_comparison_panel_datasets()
                    
                    # Audit log
                    duration = time.time() - start_time
                    summary = dataset.get_summary()
                    self.audit_logger.log_action(
                        "Dataset Loaded",
                        details={
                            'rows': summary.get('row_count', 0),
                            'type': dataset.dataset_type.value
                        },
                        duration=duration
                    )
                    
                    self.logger.info(f"GO/KEGG dataset '{dataset.name}' loaded successfully with {len(dataset.dataframe) if dataset.dataframe is not None else 0} terms")
                    return
            except Exception as e:
                self.logger.warning(f"Quick type detection failed: {e}, using standard loader")
            
            # DE ë°ì´í„°ì…‹ì€ ê¸°ì¡´ ë¡œë” ì‚¬ìš©
            # ì»¬ëŸ¼ ë§¤í•‘ ì½œë°± í•¨ìˆ˜
            def column_mapper_callback(df, dataset_type, auto_mapping):
                from gui.column_mapper_dialog import ColumnMapperDialog
                dialog = ColumnMapperDialog(df, dataset_type, auto_mapping, self.view)
                
                if dialog.exec():
                    mapping = dialog.get_mapping()
                    
                    # ì‚¬ìš©ìê°€ ì €ì¥ ì˜µì…˜ì„ ì„ íƒí•œ ê²½ìš°
                    if dialog.should_save_mapping():
                        self.data_loader.save_custom_mapping(dataset_type, mapping)
                        self.logger.info("User mapping saved for future use")
                    
                    return mapping
                else:
                    return None
            
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¡œë“œ (ë‚˜ì¤‘ì— Workerë¡œ ë³€ê²½ ê°€ëŠ¥)
            dataset = self.data_loader.load_from_excel(
                file_path, 
                final_name,  # custom_name ë˜ëŠ” dataset_name ì‚¬ìš©
                column_mapper_callback=column_mapper_callback
            )
            
            # ë°ì´í„°ì…‹ ì €ì¥ (ê³ ìœ  ì´ë¦„ ìƒì„±)
            unique_name = self.view.dataset_manager._generate_unique_name(dataset.name)
            dataset.name = unique_name
            self.datasets[unique_name] = dataset
            self.current_dataset = dataset
            
            # ìƒíƒœ ì „í™˜
            self.fsm.trigger(Event.DATA_LOAD_SUCCESS)
            
            # Signal ë°©ì¶œ
            self.dataset_loaded.emit(dataset.name, dataset)
            
            # GUI ì—…ë°ì´íŠ¸
            self._update_view_with_dataset(dataset)
            
            # Comparison panel ì—…ë°ì´íŠ¸
            self.view._update_comparison_panel_datasets()
            
            # Audit log
            duration = time.time() - start_time
            summary = dataset.get_summary()
            self.audit_logger.log_action(
                "Dataset Loaded",
                details={
                    'rows': summary.get('row_count', 0),
                    'type': dataset.dataset_type.value
                },
                duration=duration
            )
            
            self.logger.info(f"Dataset '{dataset.name}' loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load dataset: {e}", exc_info=True)
            self.fsm.trigger(Event.DATA_LOAD_FAILED)
            self.error_occurred.emit(f"Failed to load dataset: {str(e)}")
    
    def load_gene_list(self, file_path: Path):
        """ìœ ì „ì ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        try:
            genes = self.data_loader.load_gene_list_from_file(file_path)
            
            # Filter panelì— ì„¤ì •
            self.view.filter_panel.set_gene_list(genes)
            
            self.audit_logger.log_action(
                "Gene List Loaded",
                details={'file': str(file_path), 'count': len(genes)}
            )
            
            self.logger.info(f"Loaded {len(genes)} genes from file")
            
        except Exception as e:
            self.logger.error(f"Failed to load gene list: {e}")
            self.error_occurred.emit(f"Failed to load gene list: {str(e)}")
    
    def switch_dataset(self, dataset_name: str):
        """í˜„ì¬ ë°ì´í„°ì…‹ ì „í™˜"""
        if dataset_name in self.datasets:
            self.current_dataset = self.datasets[dataset_name]
            # add_to_manager=Falseë¡œ í˜¸ì¶œí•˜ì—¬ ì¤‘ë³µ ì¶”ê°€ ë°©ì§€
            self._update_view_with_dataset(self.current_dataset, add_to_manager=False)
            
            self.audit_logger.log_action(
                "Dataset Switched",
                details={'dataset': dataset_name}
            )
            
            self.logger.info(f"Switched to dataset: {dataset_name}")
        else:
            self.logger.warning(f"Dataset not found: {dataset_name}")
    
    def apply_filter(self, criteria: FilterCriteria):
        """
        í•„í„° ì ìš©
        
        Args:
            criteria: í•„í„°ë§ ê¸°ì¤€ (mode, gene_list ë˜ëŠ” í†µê³„ê°’ í¬í•¨)
        """
        import time
        start_time = time.time()
        
        if self.current_dataset is None:
            self.error_occurred.emit("No dataset loaded")
            return
        
        # ìƒíƒœ ì „í™˜
        if not self.fsm.trigger(Event.START_FILTER):
            self.logger.warning("Cannot filter in current state")
            return
        
        # Audit log
        self.audit_logger.log_action(
            "Apply Filter",
            details=criteria.to_dict()
        )
        
        try:
            # í•„í„°ë§ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
            if criteria.mode == FilterMode.GENE_LIST:
                # Gene List ëª¨ë“œ
                if not criteria.gene_list:
                    self.error_occurred.emit("Gene list is empty")
                    self.fsm.trigger(Event.FILTER_FAILED)
                    return
                
                filtered_df = self._filter_by_gene_list(criteria.gene_list)
                tab_name = f"Filtered: Gene List ({len(criteria.gene_list)} genes)"
                
            else:  # FilterMode.STATISTICAL
                # Statistical ëª¨ë“œ - ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
                dataset_type = self.current_dataset.dataset_type
                
                if dataset_type == DatasetType.DIFFERENTIAL_EXPRESSION:
                    # DE ë°ì´í„°: adj_pvalueì™€ log2fc ì‚¬ìš©
                    filtered_df = self._filter_by_statistics(
                        adj_pvalue_max=criteria.adj_pvalue_max,
                        log2fc_min=criteria.log2fc_min,
                        fdr_max=None  # DEì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨
                    )
                    tab_name = f"Filtered: pâ‰¤{criteria.adj_pvalue_max}, |FC|â‰¥{criteria.log2fc_min}"
                
                elif dataset_type == DatasetType.GO_ANALYSIS:
                    # GO ë°ì´í„°: fdr, ontology, direction ì‚¬ìš©
                    filtered_df = self._filter_by_statistics(
                        adj_pvalue_max=None,  # GOì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨
                        log2fc_min=None,      # GOì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨
                        fdr_max=criteria.fdr_max,
                        ontology=criteria.ontology,
                        go_direction=criteria.go_direction
                    )
                    # íƒ­ ì´ë¦„ì— í•„í„° ì •ë³´ í¬í•¨ (ìœ íš¨ìˆ«ì ì¤„ì´ê¸°)
                    filters = []
                    if criteria.fdr_max is not None:
                        # FDR ê°’ì„ ê³¼í•™ì  í‘œê¸°ë²• ë˜ëŠ” ì ì ˆí•œ ìë¦¿ìˆ˜ë¡œ í‘œì‹œ
                        if criteria.fdr_max < 0.001:
                            filters.append(f"FDRâ‰¤{criteria.fdr_max:.1e}")
                        else:
                            filters.append(f"FDRâ‰¤{criteria.fdr_max:.3f}")
                    if criteria.ontology != "All":
                        filters.append(criteria.ontology)
                    if criteria.go_direction != "All":
                        filters.append(criteria.go_direction)
                    tab_name = f"Filtered: {', '.join(filters)}"
                
                else:
                    self.error_occurred.emit(f"Unsupported dataset type: {dataset_type.value}")
                    self.fsm.trigger(Event.FILTER_FAILED)
                    return
            
            # ë¹ˆ ê²°ê³¼ ì²´í¬
            if filtered_df.empty:
                self.logger.warning("Filter returned no results")
                # ì—ëŸ¬ ìƒíƒœë¡œ ê°€ì§€ ì•Šê³  ì •ë³´ ë©”ì‹œì§€ë§Œ í‘œì‹œ
                from PyQt6.QtWidgets import QMessageBox
                QMessageBox.information(
                    None,
                    "No Results",
                    "No data matches the current filter criteria.\nPlease adjust your filter settings."
                )
                # SUCCESSë¡œ ì „í™˜í•˜ë˜ ë¹ˆ ê²°ê³¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì—ëŸ¬ ìƒíƒœ ì§„ì… ë°©ì§€)
                self.fsm.trigger(Event.FILTER_SUCCESS)
                return
            
            # ìƒíƒœ ì „í™˜
            self.fsm.trigger(Event.FILTER_SUCCESS)
            
            # Signal ë°©ì¶œ (GUIì—ì„œ íƒ­ ìƒì„± ì²˜ë¦¬)
            self.filter_completed.emit(filtered_df, tab_name)
            
            # Audit log
            duration = time.time() - start_time
            self.audit_logger.log_action(
                "Filtering Completed",
                details={'result_count': len(filtered_df)},
                duration=duration
            )
            
            self.logger.info(f"Filter applied: {len(filtered_df)} rows")
            
        except Exception as e:
            self.logger.error(f"Filtering failed: {e}", exc_info=True)
            self.fsm.trigger(Event.FILTER_FAILED)
            self.error_occurred.emit(f"Filtering failed: {str(e)}")
    
    def _filter_by_gene_list(self, gene_list: List[str]) -> pd.DataFrame:
        """
        ìœ ì „ì ë¦¬ìŠ¤íŠ¸ë¡œ í•„í„°ë§ (ì…ë ¥ ìˆœì„œ ìœ ì§€)
        
        Args:
            gene_list: ìœ ì „ì ID/Symbol ë¦¬ìŠ¤íŠ¸ (ìˆœì„œ ìœ ì§€ë¨)
            
        Returns:
            í•„í„°ë§ëœ DataFrame (gene_list ìˆœì„œëŒ€ë¡œ ì •ë ¬)
        """
        df = self.current_dataset.dataframe
        
        # Symbol ì»¬ëŸ¼ ìš°ì„  ì‚¬ìš© (DE ë°ì´í„°ì…‹), ì—†ìœ¼ë©´ GENE_ID ì‚¬ìš©
        if StandardColumns.SYMBOL in df.columns:
            gene_col = StandardColumns.SYMBOL
        elif StandardColumns.GENE_ID in df.columns:
            gene_col = StandardColumns.GENE_ID
        else:
            self.logger.error(f"Available columns: {df.columns.tolist()}")
            raise ValueError(f"Neither '{StandardColumns.SYMBOL}' nor '{StandardColumns.GENE_ID}' column found in dataset")
        
        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        # {ì†Œë¬¸ì ìœ ì „ìëª…: ì›ë³¸ í–‰ ì¸ë±ìŠ¤ë“¤}
        gene_to_indices = {}
        for idx, gene in enumerate(df[gene_col]):
            gene_lower = str(gene).lower()
            if gene_lower not in gene_to_indices:
                gene_to_indices[gene_lower] = []
            gene_to_indices[gene_lower].append(idx)
        
        # gene_list ìˆœì„œëŒ€ë¡œ í–‰ ìˆ˜ì§‘
        matched_indices = []
        for gene in gene_list:
            gene_lower = gene.lower()
            if gene_lower in gene_to_indices:
                matched_indices.extend(gene_to_indices[gene_lower])
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        seen = set()
        ordered_indices = []
        for idx in matched_indices:
            if idx not in seen:
                seen.add(idx)
                ordered_indices.append(idx)
        
        # DataFrame ì¬êµ¬ì„± (ìˆœì„œ ìœ ì§€)
        filtered = df.iloc[ordered_indices].copy()
        
        self.logger.info(
            f"Gene list filter: {len(filtered)}/{len(df)} rows matched, "
            f"order preserved from input list"
        )
        
        return filtered
    
    def _filter_by_statistics(self, adj_pvalue_max: Optional[float] = None, 
                               log2fc_min: Optional[float] = None, 
                               fdr_max: Optional[float] = None,
                               ontology: Optional[str] = None,
                               go_direction: Optional[str] = None) -> pd.DataFrame:
        """
        í†µê³„ê°’ìœ¼ë¡œ í•„í„°ë§ (p-value, FC)
        
        Args:
            adj_pvalue_max: ìµœëŒ€ adjusted p-value (DEìš©, optional)
            log2fc_min: ìµœì†Œ ì ˆëŒ€ log2 Fold Change (DEìš©, optional)
            fdr_max: ìµœëŒ€ FDR (GO analysisìš©, optional)
            ontology: Ontology í•„í„° (GOìš©, optional) - "All", "BP", "MF", "CC", "KEGG"
            go_direction: Direction í•„í„° (GOìš©, optional) - "All", "UP", "DOWN", "TOTAL"
            
        Returns:
            í•„í„°ë§ëœ DataFrame
        """
        dataset_type = self.current_dataset.dataset_type
        df = self.current_dataset.dataframe.copy()
        
        if dataset_type == DatasetType.DIFFERENTIAL_EXPRESSION:
            # DE ë°ì´í„° í•„í„°ë§ (í‘œì¤€ ì»¬ëŸ¼ëª… ì§ì ‘ ì‚¬ìš©)
            adj_pval_col = StandardColumns.ADJ_PVALUE
            log2fc_col = StandardColumns.LOG2FC
            
            if not all([adj_pval_col in df.columns, log2fc_col in df.columns]):
                self.logger.error(f"Available columns: {df.columns.tolist()}")
                raise ValueError(f"Required columns not found: {adj_pval_col}, {log2fc_col}")
            
            mask = (df[adj_pval_col] <= adj_pvalue_max) & (abs(df[log2fc_col]) >= log2fc_min)
            filtered = df[mask]
            
            self.logger.info(
                f"Statistical filter (DE): {len(filtered)}/{len(df)} rows "
                f"(pâ‰¤{adj_pvalue_max}, |FC|â‰¥{log2fc_min})"
            )
            
        elif dataset_type == DatasetType.GO_ANALYSIS:
            # GO ë°ì´í„° í•„í„°ë§ (í‘œì¤€ ì»¬ëŸ¼ëª… ì§ì ‘ ì‚¬ìš©)
            fdr_col = StandardColumns.FDR
            
            if fdr_col not in df.columns:
                self.logger.error(f"Available columns: {df.columns.tolist()}")
                raise ValueError(f"FDR column not found: {fdr_col}")
            
            # FDR í•„í„°
            mask = df[fdr_col] <= fdr_max
            
            # Ontology í•„í„°
            if ontology and ontology != "All":
                ontology_col = StandardColumns.ONTOLOGY
                if ontology_col in df.columns:
                    mask = mask & (df[ontology_col].str.upper() == ontology.upper())
            
            # Gene Set í•„í„° (UP/DOWN/TOTAL DEG)
            if go_direction and go_direction != "All":
                gene_set_col = StandardColumns.GENE_SET
                if gene_set_col in df.columns:
                    mask = mask & (df[gene_set_col].str.upper() == go_direction.upper())
            
            filtered = df[mask]
            
            filter_desc = f"FDRâ‰¤{fdr_max}"
            if ontology and ontology != "All":
                filter_desc += f", {ontology}"
            if go_direction and go_direction != "All":
                filter_desc += f", {go_direction}"
            
            self.logger.info(
                f"Statistical filter (GO): {len(filtered)}/{len(df)} rows ({filter_desc})"
            )
        else:
            raise ValueError(f"Cannot filter dataset type: {dataset_type.value}")
        
        return filtered
    
    def run_analysis(self, analysis_type: str, gene_list: List[str], 
                     adj_pvalue_cutoff: float = 0.05, log2fc_cutoff: float = 1.0):
        """
        í†µê³„ ë¶„ì„ ì‹¤í–‰
        
        Args:
            analysis_type: ë¶„ì„ íƒ€ì… ("fisher", "gsea")
            gene_list: ê´€ì‹¬ ìœ ì „ì ë¦¬ìŠ¤íŠ¸
            adj_pvalue_cutoff: Adjusted p-value ì„ê³„ê°’ (Fisher's testìš©)
            log2fc_cutoff: log2 Fold Change ì„ê³„ê°’ (Fisher's testìš©)
        """
        import time
        start_time = time.time()
        
        if self.current_dataset is None:
            self.error_occurred.emit("No dataset loaded")
            return
        
        if not gene_list:
            self.error_occurred.emit("No genes provided for analysis")
            return
        
        # ìƒíƒœ ì „í™˜
        if not self.fsm.trigger(Event.START_ANALYSIS):
            self.logger.warning("Cannot analyze in current state")
            return
        
        # Audit log
        self.audit_logger.log_action(
            f"{analysis_type.upper()} Analysis",
            details={'gene_count': len(gene_list)}
        )
        
        try:
            # ë¶„ì„ ì‹¤í–‰
            if analysis_type == "fisher":
                result = self.analyzer.fisher_exact_test(
                    gene_list, self.current_dataset,
                    adj_pvalue_cutoff=adj_pvalue_cutoff,
                    log2fc_cutoff=log2fc_cutoff
                )
            elif analysis_type == "gsea":
                result = self.analyzer.gsea_lite(
                    gene_list, self.current_dataset,
                    adj_pvalue_cutoff=adj_pvalue_cutoff
                )
            else:
                raise ValueError(f"Unknown analysis type: {analysis_type}")
            
            # ìƒíƒœ ì „í™˜
            self.fsm.trigger(Event.ANALYSIS_SUCCESS)
            
            # Signal ë°©ì¶œ
            self.analysis_completed.emit(result, analysis_type)
            
            # ë¶„ì„ ë¡œê·¸ ì €ì¥
            log_file_path = self._save_analysis_log(analysis_type, gene_list, result, 
                                                     adj_pvalue_cutoff, log2fc_cutoff)
            
            # ê²°ê³¼ í‘œì‹œ (ë¡œê·¸ íŒŒì¼ ê²½ë¡œ í¬í•¨)
            self._show_analysis_result(result, analysis_type, log_file_path)
            
            # Audit log
            duration = time.time() - start_time
            self.audit_logger.log_action(
                f"{analysis_type.upper()} Completed",
                details={'pvalue': result.get('pvalue', 'N/A')},
                duration=duration
            )
            
            self.logger.info(f"{analysis_type} analysis completed")
            
        except Exception as e:
            self.logger.error(f"Analysis failed: {e}", exc_info=True)
            self.fsm.trigger(Event.ANALYSIS_FAILED)
            self.error_occurred.emit(f"Analysis failed: {str(e)}")
    
    def compare_datasets(self, dataset_names: List[str]):
        """
        ë‹¤ì¤‘ ë°ì´í„°ì…‹ ë¹„êµ
        
        Args:
            dataset_names: ë¹„êµí•  ë°ì´í„°ì…‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        import time
        start_time = time.time()
        
        # ìƒíƒœ ì „í™˜
        if not self.fsm.trigger(Event.START_COMPARISON):
            self.logger.warning("Cannot compare in current state")
            return
        
        # Audit log
        self.audit_logger.log_action(
            "Compare Datasets",
            details={'datasets': dataset_names, 'count': len(dataset_names)}
        )
        
        try:
            # ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
            datasets = [self.datasets[name] for name in dataset_names 
                       if name in self.datasets]
            
            if len(datasets) < 2:
                raise ValueError("At least 2 datasets required for comparison")
            
            # ë¹„êµ ì‹¤í–‰
            result = self.analyzer.compare_datasets(datasets)
            
            # ìƒíƒœ ì „í™˜
            self.fsm.trigger(Event.COMPARISON_SUCCESS)
            
            # Signal ë°©ì¶œ
            self.comparison_completed.emit(result)
            
            # ê²°ê³¼ í‘œì‹œ
            self._show_comparison_result(result)
            
            # Audit log
            duration = time.time() - start_time
            self.audit_logger.log_action(
                "Comparison Completed",
                details={
                    'common_genes': len(result.common_genes),
                    'total_genes': result.metadata.get('total_genes', 0)
                },
                duration=duration
            )
            
            self.logger.info("Dataset comparison completed")
            
        except Exception as e:
            self.logger.error(f"Comparison failed: {e}", exc_info=True)
            self.fsm.trigger(Event.COMPARISON_FAILED)
            self.error_occurred.emit(f"Comparison failed: {str(e)}")
    
    def export_data(self, file_path: Path, table_widget):
        """
        ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        
        Args:
            file_path: ì €ì¥ ê²½ë¡œ
            table_widget: QTableWidget
        """
        import time
        start_time = time.time()
        
        # Audit log
        self.audit_logger.log_action(
            "Export Data",
            details={'file': str(file_path)}
        )
        
        try:
            # QTableWidgetì—ì„œ ë°ì´í„° ì¶”ì¶œ
            df = self._table_to_dataframe(table_widget)
            
            # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ì €ì¥
            if file_path.suffix == '.csv':
                df.to_csv(file_path, index=False)
            elif file_path.suffix == '.tsv':
                df.to_csv(file_path, sep='\t', index=False)
            elif file_path.suffix in ['.xlsx', '.xls']:
                df.to_excel(file_path, index=False)
            else:
                raise ValueError(f"Unsupported file format: {file_path.suffix}")
            
            # Audit log
            duration = time.time() - start_time
            self.audit_logger.log_action(
                "Export Completed",
                details={'rows': len(df), 'format': file_path.suffix},
                duration=duration
            )
            
            self.logger.info(f"Data exported to {file_path}")
            
        except Exception as e:
            self.logger.error(f"Export failed: {e}", exc_info=True)
            self.error_occurred.emit(f"Export failed: {str(e)}")
    
    def _update_view_with_dataset(self, dataset: Dataset, add_to_manager: bool = True):
        """
        ë°ì´í„°ì…‹ìœ¼ë¡œ ë·° ì—…ë°ì´íŠ¸
        
        Args:
            dataset: ì—…ë°ì´íŠ¸í•  Dataset ê°ì²´
            add_to_manager: Dataset Managerì— ì¶”ê°€ ì—¬ë¶€ (Falseë©´ ê¸°ì¡´ í•­ëª© ìœ ì§€)
        """
        if dataset.dataframe is not None:
            # "Whole Dataset" íƒ­ ì°¾ê¸° ë° ì—…ë°ì´íŠ¸
            for i in range(self.view.data_tabs.count()):
                tab_name = self.view.data_tabs.tabText(i)
                if tab_name == "Whole Dataset":
                    table = self.view.data_tabs.widget(i)
                    if table:
                        self.view.populate_table(table, dataset.dataframe, dataset)
                    break
            else:
                # "Whole Dataset" íƒ­ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ íƒ­ ì—…ë°ì´íŠ¸
                table = self.view.data_tabs.widget(0)
                if table:
                    self.view.populate_table(table, dataset.dataframe, dataset)
            
            # Dataset manager ì—…ë°ì´íŠ¸ (ì‹ ê·œ ë¡œë“œ ì‹œì—ë§Œ)
            if add_to_manager:
                metadata = {
                    'file_path': str(dataset.file_path) if dataset.file_path else '',
                    'dataset_type': dataset.dataset_type.value,
                    'row_count': len(dataset.dataframe),
                    'column_count': len(dataset.dataframe.columns)
                }
                self.view.dataset_manager.add_dataset(dataset.name, metadata=metadata)
    
    def _create_result_tab(self, df: pd.DataFrame, tab_name: str):
        """ê²°ê³¼ íƒ­ ìƒì„±"""
        table = self.view._create_data_tab(tab_name)
        # í˜„ì¬ ë°ì´í„°ì…‹ ì •ë³´ ì „ë‹¬
        self.view.populate_table(table, df, self.current_dataset)
        
        # ìƒˆ íƒ­ìœ¼ë¡œ ì „í™˜
        self.view.data_tabs.setCurrentWidget(table)
    
    def _table_to_dataframe(self, table_widget) -> pd.DataFrame:
        """QTableWidgetë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        rows = table_widget.rowCount()
        cols = table_widget.columnCount()
        
        # í—¤ë”
        headers = [table_widget.horizontalHeaderItem(i).text() 
                  for i in range(cols)]
        
        # ë°ì´í„°
        data = []
        for i in range(rows):
            row_data = []
            for j in range(cols):
                item = table_widget.item(i, j)
                row_data.append(item.text() if item else "")
            data.append(row_data)
        
        return pd.DataFrame(data, columns=headers)
    
    def _save_analysis_log(self, analysis_type: str, gene_list: List[str], 
                          result: dict, adj_pvalue_cutoff: float, log2fc_cutoff: float):
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            analysis_type: ë¶„ì„ íƒ€ì…
            gene_list: ì…ë ¥ ìœ ì „ì ë¦¬ìŠ¤íŠ¸
            result: ë¶„ì„ ê²°ê³¼
            adj_pvalue_cutoff: p-value ì„ê³„ê°’
            log2fc_cutoff: log2FC ì„ê³„ê°’
        """
        from datetime import datetime
        from pathlib import Path
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = Path("analysis_logs")
        log_dir.mkdir(exist_ok=True)
        
        # ë¡œê·¸ íŒŒì¼ëª…: analysis_type_YYYYMMDD_HHMMSS.txt
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{analysis_type}_{timestamp}.txt"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("=" * 80 + "\n")
                f.write(f"{analysis_type.upper()} ANALYSIS LOG\n")
                f.write("=" * 80 + "\n\n")
                
                # ë¶„ì„ ì •ë³´
                f.write(f"Date/Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Analysis Type: {analysis_type.upper()}\n")
                f.write(f"Dataset: {self.current_dataset.name}\n")
                f.write(f"Dataset Type: {self.current_dataset.dataset_type.value}\n\n")
                
                # ì„¤ì • ì •ë³´
                f.write("-" * 80 + "\n")
                f.write("SETTINGS\n")
                f.write("-" * 80 + "\n")
                f.write(f"Adjusted p-value cutoff: {adj_pvalue_cutoff}\n")
                f.write(f"log2 Fold Change cutoff: {log2fc_cutoff}\n\n")
                
                # ì…ë ¥ ìœ ì „ì ë¦¬ìŠ¤íŠ¸
                f.write("-" * 80 + "\n")
                f.write(f"INPUT GENE LIST ({len(gene_list)} genes)\n")
                f.write("-" * 80 + "\n")
                for gene in gene_list:
                    f.write(f"{gene}\n")
                f.write("\n")
                
                # ë¶„ì„ ê²°ê³¼
                f.write("-" * 80 + "\n")
                f.write("RESULTS\n")
                f.write("-" * 80 + "\n")
                
                if analysis_type == "fisher":
                    f.write(f"P-value: {result['pvalue']:.4e}\n")
                    f.write(f"Odds Ratio: {result['odds_ratio']:.2f}\n")
                    f.write(f"Enrichment Fold: {result['enrichment_fold']:.2f}\n")
                    f.write(f"Significant: {'Yes' if result['significant'] else 'No'}\n\n")
                    
                    f.write(f"In Gene List (Significant): {result['in_list_significant']}\n")
                    f.write(f"In Gene List (Total): {result['in_list_total']}\n")
                    f.write(f"Dataset (Significant): {result['dataset_significant']}\n")
                    f.write(f"Dataset (Total): {result['dataset_total']}\n\n")
                    
                    f.write("Contingency Table:\n")
                    table = result['contingency_table']
                    f.write(f"  In List & Significant:     {table[0][0]}\n")
                    f.write(f"  In List & Not Significant: {table[0][1]}\n")
                    f.write(f"  Not in List & Significant: {table[1][0]}\n")
                    f.write(f"  Not in List & Not Sig:     {table[1][1]}\n")
                    
                elif analysis_type == "gsea":
                    f.write(f"Mean log2FC: {result['mean_log2fc']:.3f}\n")
                    f.write(f"Median log2FC: {result['median_log2fc']:.3f}\n")
                    f.write(f"Wilcoxon p-value: {result['wilcoxon_pvalue']:.4e}\n")
                    f.write(f"Enrichment Direction: {result['enrichment_direction']}\n\n")
                    
                    f.write(f"Up-regulated (significant): {result['upregulated_count']}\n")
                    f.write(f"Down-regulated (significant): {result['downregulated_count']}\n")
                    f.write(f"Total significant: {result['significant_count']}\n")
                    f.write(f"Total genes in list: {result['total_count']}\n")
                
                f.write("\n" + "=" * 80 + "\n")
            
            self.logger.info(f"Analysis log saved to {log_file}")
            return str(log_file.absolute())
            
        except Exception as e:
            self.logger.error(f"Failed to save analysis log: {e}", exc_info=True)
            return None
    
    def _show_analysis_result(self, result: dict, analysis_type: str, log_file_path: Optional[str] = None):
        """ë¶„ì„ ê²°ê³¼ í‘œì‹œ (ë‹¤ì´ì–¼ë¡œê·¸)"""
        from PyQt6.QtWidgets import QMessageBox
        
        if analysis_type == "fisher":
            # Prepare HTML fragments outside f-string to avoid backslash issues
            sig_span = '<span style="color: green;">Significant</span>'
            not_sig_span = '<span style="color: red;">Not significant</span>'
            result_text = sig_span if result['significant'] else not_sig_span
            
            message = (
                f"<h3>Fisher's Exact Test Result</h3>"
                f"<p><b>P-value:</b> {result['pvalue']:.4e}</p>"
                f"<p><b>Odds Ratio:</b> {result['odds_ratio']:.2f}</p>"
                f"<p><b>Significant genes in list:</b> {result['in_list_significant']} / {result['in_list_total']}</p>"
                f"<p><b>Total significant genes:</b> {result['dataset_significant']} / {result['dataset_total']}</p>"
                f"<p><b>Enrichment fold:</b> {result['enrichment_fold']:.2f}</p>"
                f"<p><b>Result:</b> {result_text}</p>"
            )
            
            if log_file_path:
                message += f"<hr><p><small>ğŸ“ Analysis log saved to:<br><code>{log_file_path}</code></small></p>"
        elif analysis_type == "gsea":
            message = (
                f"<h3>GSEA Lite Result</h3>"
                f"<p><b>Mean log2FC:</b> {result['mean_log2fc']:.2f}</p>"
                f"<p><b>Median log2FC:</b> {result['median_log2fc']:.2f}</p>"
                f"<p><b>Upregulated:</b> {result['upregulated_count']}</p>"
                f"<p><b>Downregulated:</b> {result['downregulated_count']}</p>"
                f"<p><b>Significant:</b> {result['significant_count']} / {result['total_count']}</p>"
                f"<p><b>Enrichment direction:</b> {result['enrichment_direction']}</p>"
                f"<p><b>Wilcoxon p-value:</b> {result.get('wilcoxon_pvalue', 'N/A')}</p>"
            )
            
            if log_file_path:
                message += f"<hr><p><small>ğŸ“ Analysis log saved to:<br><code>{log_file_path}</code></small></p>"
        else:
            message = str(result)
        
        msg_box = QMessageBox(self.view)
        msg_box.setWindowTitle(f"{analysis_type.upper()} Analysis Result")
        msg_box.setTextFormat(Qt.TextFormat.RichText)
        msg_box.setText(message)
        msg_box.exec()
    
    def _show_comparison_result(self, result: ComparisonResult):
        """ë¹„êµ ê²°ê³¼ í‘œì‹œ"""
        if result.comparison_table is not None:
            tab_name = "Comparison Result"
            self._create_result_tab(result.comparison_table, tab_name)
    
    # ========== GO/KEGG Analysis Methods ==========
    
    def load_go_kegg_data(self, file_paths: List[Path], is_excel: bool = True, 
                          dataset_name: str = "GO/KEGG Analysis"):
        """
        GO/KEGG ë¶„ì„ ê²°ê³¼ ë¡œë”©
        
        Args:
            file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            is_excel: Excel íŒŒì¼ ì—¬ë¶€ (Falseë©´ CSV íŒŒì¼ë“¤)
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        """
        try:
            from utils.go_kegg_loader import GOKEGGLoader
            
            loader = GOKEGGLoader()
            
            if is_excel:
                # ë‹¨ì¼ Excel íŒŒì¼
                dataset = loader.load_from_excel(file_paths[0], name=dataset_name)
            else:
                # ì—¬ëŸ¬ CSV íŒŒì¼
                dataset = loader.load_from_csv_files(file_paths, name=dataset_name)
            
            # ë°ì´í„°ì…‹ ì €ì¥ (ê³ ìœ  ì´ë¦„ ìƒì„±)
            unique_name = self.view.dataset_manager._generate_unique_name(dataset.name)
            dataset.name = unique_name
            self.datasets[unique_name] = dataset
            self.current_dataset = dataset
            
            # GUI ì—…ë°ì´íŠ¸ (Whole Dataset íƒ­ì— í‘œì‹œ)
            self._update_view_with_dataset(dataset)
            
            # Comparison panel ì—…ë°ì´íŠ¸
            self.view._update_comparison_panel_datasets()
            
            # ì‹ í˜¸ ë°œìƒ
            self.dataset_loaded.emit(dataset.name, dataset)
            
            n_terms = len(dataset.dataframe) if dataset.dataframe is not None else 0
            self.logger.info(f"GO/KEGG data loaded: {n_terms} terms")
            self.audit_logger.log_action("GO/KEGG Data Loaded", 
                                        details={"name": dataset.name, "rows": n_terms})
            
        except Exception as e:
            self.logger.error(f"Failed to load GO/KEGG data: {e}", exc_info=True)
            self.error_occurred.emit(f"Failed to load GO/KEGG data:\n{str(e)}")
    
    def cluster_go_terms(self, dataset: Dataset, kappa_threshold: float = 0.4,
                         total_genes: Optional[int] = None):
        """
        GO Term í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘
        
        Args:
            dataset: GO/KEGG ë°ì´í„°ì…‹
            kappa_threshold: Kappa statistic ì„ê³„ê°’
            total_genes: ì „ì²´ ìœ ì „ì ìˆ˜
        """
        if dataset.dataset_type != DatasetType.GO_ANALYSIS:
            self.error_occurred.emit("Clustering is only available for GO/KEGG datasets")
            return
        
        if dataset.dataframe is None:
            self.error_occurred.emit("Dataset has no data")
            return
        
        try:
            # FSM ìƒíƒœ ì „í™˜
            self.fsm.trigger(Event.START_CLUSTERING)
            
            # Worker ì‹œì‘
            from workers.go_workers import GOClusteringWorker
            
            self.clustering_worker = GOClusteringWorker(
                df=dataset.dataframe,
                kappa_threshold=kappa_threshold,
                total_genes=total_genes
            )
            
            self.clustering_worker.progress.connect(self.progress_updated.emit)
            self.clustering_worker.finished.connect(self._on_clustering_finished)
            self.clustering_worker.error.connect(self._on_clustering_error)
            
            self.clustering_worker.start()
            
            self.logger.info(f"Started GO term clustering (threshold={kappa_threshold})")
            
        except Exception as e:
            self.logger.error(f"Failed to start clustering: {e}", exc_info=True)
            self.fsm.trigger(Event.CLUSTERING_FAILED)
            self.error_occurred.emit(f"Failed to start clustering:\n{str(e)}")
    
    def _on_clustering_finished(self, clustered_df: pd.DataFrame, clusters: Dict):
        """í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ ì²˜ë¦¬"""
        try:
            # FSM ìƒíƒœ ì „í™˜ (í˜„ì¬ ìƒíƒœê°€ CLUSTERINGì¸ ê²½ìš°ì—ë§Œ)
            try:
                self.fsm.trigger(Event.CLUSTERING_SUCCESS)
            except Exception as fsm_error:
                self.logger.warning(f"FSM transition ignored: {fsm_error}")
            
            # cluster_id ì»¬ëŸ¼ í™•ì¸ ë° ì •ë ¬
            if 'cluster_id' in clustered_df.columns:
                clustered_df = clustered_df.sort_values('cluster_id', ascending=True)
            
            # ê²°ê³¼ë¥¼ ìƒˆ íƒ­ì— í‘œì‹œ - "Clustered: " ì ‘ë‘ì‚¬ ì‚¬ìš©
            tab_name = f"Clustered: {len(clusters)} clusters"
            
            # Viewì— ê²°ê³¼ ì „ë‹¬ (filter_completed ì‹œê·¸ë„ ì¬ì‚¬ìš©)
            self.filter_completed.emit(clustered_df, tab_name)
            
            self.logger.info(f"Clustering completed: {len(clusters)} clusters from {len(clustered_df)} terms")
            self.audit_logger.log_action("GO Clustering Completed", 
                                        details={"n_clusters": len(clusters), "n_terms": len(clustered_df)})
            
            # ê²°ê³¼ ë‹¤ì´ì–¼ë¡œê·¸ í‘œì‹œ
            from PyQt6.QtWidgets import QMessageBox
            QMessageBox.information(
                self.view,
                "Clustering Complete",
                f"Successfully clustered {len(clustered_df)} GO terms into {len(clusters)} clusters.\n\n"
                f"Results sorted by cluster and displayed in new tab: '{tab_name}'"
            )
            
        except Exception as e:
            self.logger.error(f"Failed to process clustering results: {e}", exc_info=True)
            self.error_occurred.emit(f"Failed to process clustering results:\n{str(e)}")
    
    def _on_clustering_error(self, error_message: str):
        """í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜ ì²˜ë¦¬"""
        self.fsm.trigger(Event.CLUSTERING_FAILED)
        self.logger.error(f"Clustering error: {error_message}")
        self.error_occurred.emit(f"Clustering failed:\n{error_message}")
    
    def filter_go_kegg_data(
        self,
        dataset: Dataset,
        fdr_threshold: Optional[float] = None,
        ontologies: Optional[List[str]] = None,
        direction: Optional[str] = None,
        gene_count_range: Optional[tuple] = None,
        description_filter: Optional[tuple] = None
    ):
        """
        GO/KEGG ë°ì´í„° í•„í„°ë§
        
        Args:
            dataset: í•„í„°ë§í•  Dataset
            fdr_threshold: FDR ì„ê³„ê°’ (Noneì´ë©´ í•„í„°ë§í•˜ì§€ ì•ŠìŒ)
            ontologies: í¬í•¨í•  Ontology ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['BP', 'CC'])
            direction: ë°©í–¥ í•„í„° ('UP', 'DOWN', 'TOTAL', None=ëª¨ë‘)
            gene_count_range: Gene count ë²”ìœ„ (min, max) íŠœí”Œ
            description_filter: Description ê²€ìƒ‰ (keyword, case_sensitive) íŠœí”Œ
        """
        try:
            if dataset.dataframe is None:
                raise ValueError("Dataset has no data")
            
            df = dataset.dataframe.copy()
            original_count = len(df)
            
            # FDR í•„í„°
            if fdr_threshold is not None and StandardColumns.FDR in df.columns:
                df = df[df[StandardColumns.FDR] <= fdr_threshold]
                self.logger.debug(f"After FDR filter: {len(df)} terms")
            
            # Ontology í•„í„°
            if ontologies and StandardColumns.ONTOLOGY in df.columns:
                df = df[df[StandardColumns.ONTOLOGY].isin(ontologies)]
                self.logger.debug(f"After Ontology filter: {len(df)} terms")
            
            # Direction í•„í„°
            if direction and StandardColumns.DIRECTION in df.columns:
                df = df[df[StandardColumns.DIRECTION] == direction]
                self.logger.debug(f"After Direction filter: {len(df)} terms")
            
            # Gene count ë²”ìœ„ í•„í„°
            if gene_count_range and StandardColumns.GENE_COUNT in df.columns:
                min_genes, max_genes = gene_count_range
                df = df[
                    (df[StandardColumns.GENE_COUNT] >= min_genes) &
                    (df[StandardColumns.GENE_COUNT] <= max_genes)
                ]
                self.logger.debug(f"After gene count filter: {len(df)} terms")
            
            # Description ê²€ìƒ‰ í•„í„°
            if description_filter and StandardColumns.DESCRIPTION in df.columns:
                keyword, case_sensitive = description_filter
                if keyword:
                    df = df[df[StandardColumns.DESCRIPTION].str.contains(
                        keyword, case=case_sensitive, na=False
                    )]
                    self.logger.debug(f"After description filter: {len(df)} terms")
            
            # ê²°ê³¼ ë¡œê¹…
            filtered_count = len(df)
            self.logger.info(
                f"GO/KEGG filtering: {original_count} â†’ {filtered_count} terms "
                f"({filtered_count/original_count*100:.1f}%)"
            )
            
            # Audit log
            self.audit_logger.log_action(
                action="filter_go_kegg",
                details={
                    "dataset": dataset.name,
                    "original_count": original_count,
                    "filtered_count": filtered_count,
                    "fdr_threshold": fdr_threshold,
                    "ontologies": ontologies,
                    "direction": direction,
                    "gene_count_range": gene_count_range,
                    "description_keyword": description_filter[0] if description_filter else None
                }
            )
            
            # íƒ­ ì´ë¦„ ìƒì„± (Advanced Filterì™€ ë™ì¼í•œ í˜•ì‹)
            filters = []
            if fdr_threshold is not None:
                # FDR ê°’ì„ ê³¼í•™ì  í‘œê¸°ë²• ë˜ëŠ” ì ì ˆí•œ ìë¦¿ìˆ˜ë¡œ í‘œì‹œ
                if fdr_threshold < 0.001:
                    filters.append(f"FDRâ‰¤{fdr_threshold:.1e}")
                else:
                    filters.append(f"FDRâ‰¤{fdr_threshold:.3f}")
            if ontologies and len(ontologies) < 4:  # ì „ì²´ ì„ íƒì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                filters.append("+".join(ontologies))
            if direction:
                filters.append(direction)
            if gene_count_range:
                min_g, max_g = gene_count_range
                filters.append(f"genes:{min_g}-{max_g}")
            if description_filter and description_filter[0]:
                keyword = description_filter[0][:20]  # ìµœëŒ€ 20ì
                filters.append(f'"{keyword}"')
            
            # í•„í„° ì¡°ê±´ì´ ìˆìœ¼ë©´ í‘œì‹œ, ì—†ìœ¼ë©´ "All"
            if filters:
                tab_name = f"Filtered: {', '.join(filters)}"
            else:
                tab_name = f"Filtered: All ({filtered_count} terms)"
            
            self.filter_completed.emit(df, tab_name)
            
        except Exception as e:
            error_msg = f"Failed to filter GO/KEGG data: {str(e)}"
            self.logger.exception(error_msg)
            self.error_occurred.emit(error_msg)
